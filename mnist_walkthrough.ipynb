{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ”¢ MNIST Handwritten Digit Recognition\n",
                "\n",
                "A step-by-step walkthrough of building a **Convolutional Neural Network (CNN)** to classify handwritten digits.\n",
                "\n",
                "This notebook covers:\n",
                "1. Understanding the MNIST Dataset\n",
                "2. Exploring the CNN Architecture\n",
                "3. Training the Model\n",
                "4. Making Predictions\n",
                "5. Visualizing Results"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ“¦ Setup & Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from torchvision import datasets, transforms\n",
                "from torch.utils.data import DataLoader\n",
                "\n",
                "# Check device\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1ï¸âƒ£ Understanding the MNIST Dataset\n",
                "\n",
                "MNIST is a dataset of 70,000 handwritten digit images:\n",
                "- **60,000** training images\n",
                "- **10,000** test images\n",
                "- Each image is **28Ã—28 pixels**, grayscale\n",
                "- **10 classes** (digits 0-9)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load MNIST dataset\n",
                "transform = transforms.Compose([\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n",
                "])\n",
                "\n",
                "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
                "test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
                "\n",
                "print(f\"Training samples: {len(train_dataset):,}\")\n",
                "print(f\"Test samples: {len(test_dataset):,}\")\n",
                "print(f\"Image shape: {train_dataset[0][0].shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Visualizing Sample Digits"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display sample images\n",
                "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
                "\n",
                "for i, ax in enumerate(axes.flat):\n",
                "    image, label = train_dataset[i]\n",
                "    ax.imshow(image.squeeze(), cmap='gray')\n",
                "    ax.set_title(f'Label: {label}', fontsize=12)\n",
                "    ax.axis('off')\n",
                "\n",
                "plt.suptitle('Sample MNIST Digits', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Class Distribution"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Count each digit\n",
                "labels = [train_dataset[i][1] for i in range(len(train_dataset))]\n",
                "unique, counts = np.unique(labels, return_counts=True)\n",
                "\n",
                "plt.figure(figsize=(10, 5))\n",
                "plt.bar(unique, counts, color='steelblue', edgecolor='black')\n",
                "plt.xlabel('Digit', fontsize=12)\n",
                "plt.ylabel('Count', fontsize=12)\n",
                "plt.title('Distribution of Digits in Training Set', fontsize=14)\n",
                "plt.xticks(range(10))\n",
                "plt.grid(axis='y', alpha=0.3)\n",
                "plt.show()\n",
                "\n",
                "print(\"Samples per digit:\")\n",
                "for digit, count in zip(unique, counts):\n",
                "    print(f\"  {digit}: {count:,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2ï¸âƒ£ CNN Architecture\n",
                "\n",
                "Our CNN has:\n",
                "- **3 Convolutional Blocks** (Conv â†’ BatchNorm â†’ ReLU â†’ MaxPool â†’ Dropout)\n",
                "- **3 Fully Connected Layers** with dropout\n",
                "- ~300K parameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from model import MNISTNet, get_model_summary\n",
                "\n",
                "# Create model\n",
                "model = MNISTNet().to(device)\n",
                "\n",
                "# Print summary\n",
                "print(get_model_summary(model))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Print layer details\n",
                "print(\"Model Architecture:\")\n",
                "print(\"=\"*50)\n",
                "for name, layer in model.named_children():\n",
                "    params = sum(p.numel() for p in layer.parameters())\n",
                "    print(f\"{name:15} | {str(layer.__class__.__name__):20} | {params:,} params\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Visualizing a Forward Pass"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get a sample image\n",
                "sample_image, sample_label = train_dataset[0]\n",
                "sample_batch = sample_image.unsqueeze(0).to(device)\n",
                "\n",
                "print(f\"Input shape: {sample_batch.shape}\")\n",
                "\n",
                "# Forward pass\n",
                "model.eval()\n",
                "with torch.no_grad():\n",
                "    output = model(sample_batch)\n",
                "    probabilities = F.softmax(output, dim=1)\n",
                "    prediction = output.argmax(dim=1).item()\n",
                "\n",
                "print(f\"Output shape: {output.shape}\")\n",
                "print(f\"\\nPredicted digit: {prediction}\")\n",
                "print(f\"Actual label: {sample_label}\")\n",
                "print(f\"\\nProbabilities:\")\n",
                "for i, prob in enumerate(probabilities[0]):\n",
                "    bar = 'â–ˆ' * int(prob * 20)\n",
                "    print(f\"  {i}: {bar} {prob*100:.1f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3ï¸âƒ£ Training the Model\n",
                "\n",
                "We'll train for a few epochs and visualize the learning process."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create data loaders\n",
                "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
                "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
                "\n",
                "print(f\"Training batches: {len(train_loader)}\")\n",
                "print(f\"Test batches: {len(test_loader)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training function\n",
                "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
                "    model.train()\n",
                "    total_loss = 0\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    \n",
                "    for images, labels in train_loader:\n",
                "        images, labels = images.to(device), labels.to(device)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        outputs = model(images)\n",
                "        loss = criterion(outputs, labels)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        total_loss += loss.item() * images.size(0)\n",
                "        _, predicted = outputs.max(1)\n",
                "        total += labels.size(0)\n",
                "        correct += predicted.eq(labels).sum().item()\n",
                "    \n",
                "    return total_loss / total, 100. * correct / total\n",
                "\n",
                "def evaluate(model, test_loader, criterion, device):\n",
                "    model.eval()\n",
                "    total_loss = 0\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for images, labels in test_loader:\n",
                "            images, labels = images.to(device), labels.to(device)\n",
                "            outputs = model(images)\n",
                "            loss = criterion(outputs, labels)\n",
                "            \n",
                "            total_loss += loss.item() * images.size(0)\n",
                "            _, predicted = outputs.max(1)\n",
                "            total += labels.size(0)\n",
                "            correct += predicted.eq(labels).sum().item()\n",
                "    \n",
                "    return total_loss / total, 100. * correct / total"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize fresh model\n",
                "model = MNISTNet().to(device)\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
                "\n",
                "# Training loop\n",
                "epochs = 5\n",
                "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
                "\n",
                "print(\"Starting training...\\n\")\n",
                "print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Train Acc':^10} | {'Val Loss':^12} | {'Val Acc':^10}\")\n",
                "print(\"-\" * 60)\n",
                "\n",
                "for epoch in range(1, epochs + 1):\n",
                "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
                "    val_loss, val_acc = evaluate(model, test_loader, criterion, device)\n",
                "    \n",
                "    history['train_loss'].append(train_loss)\n",
                "    history['train_acc'].append(train_acc)\n",
                "    history['val_loss'].append(val_loss)\n",
                "    history['val_acc'].append(val_acc)\n",
                "    \n",
                "    print(f\"{epoch:^7} | {train_loss:^12.4f} | {train_acc:^10.2f}% | {val_loss:^12.4f} | {val_acc:^10.2f}%\")\n",
                "\n",
                "print(\"-\" * 60)\n",
                "print(f\"\\nâœ… Final Validation Accuracy: {val_acc:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Training Curves"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Loss\n",
                "ax1.plot(history['train_loss'], 'o-', label='Training', color='#2ecc71', linewidth=2)\n",
                "ax1.plot(history['val_loss'], 's-', label='Validation', color='#e74c3c', linewidth=2)\n",
                "ax1.set_xlabel('Epoch')\n",
                "ax1.set_ylabel('Loss')\n",
                "ax1.set_title('Training & Validation Loss', fontweight='bold')\n",
                "ax1.legend()\n",
                "ax1.grid(alpha=0.3)\n",
                "\n",
                "# Accuracy\n",
                "ax2.plot(history['train_acc'], 'o-', label='Training', color='#2ecc71', linewidth=2)\n",
                "ax2.plot(history['val_acc'], 's-', label='Validation', color='#e74c3c', linewidth=2)\n",
                "ax2.set_xlabel('Epoch')\n",
                "ax2.set_ylabel('Accuracy (%)')\n",
                "ax2.set_title('Training & Validation Accuracy', fontweight='bold')\n",
                "ax2.legend()\n",
                "ax2.grid(alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4ï¸âƒ£ Making Predictions\n",
                "\n",
                "Let's use our trained model (or load the pre-trained one) to make predictions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from predict import MNISTPredictor\n",
                "\n",
                "# Load the pre-trained model\n",
                "predictor = MNISTPredictor('./models/mnist_cnn.pth')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get some test samples\n",
                "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
                "images, labels = next(iter(test_loader))\n",
                "\n",
                "# Make predictions\n",
                "predictions, confidences, probs = predictor.predict_batch(images)\n",
                "\n",
                "# Visualize\n",
                "fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
                "\n",
                "for i, ax in enumerate(axes.flat):\n",
                "    img = images[i].squeeze().numpy()\n",
                "    img = img * 0.3081 + 0.1307  # Denormalize\n",
                "    \n",
                "    ax.imshow(img, cmap='gray')\n",
                "    \n",
                "    correct = predictions[i] == labels[i].item()\n",
                "    color = '#2ecc71' if correct else '#e74c3c'\n",
                "    \n",
                "    ax.set_title(f'Pred: {predictions[i]} | True: {labels[i]}\\nConf: {confidences[i]*100:.1f}%',\n",
                "                 color=color, fontweight='bold')\n",
                "    ax.axis('off')\n",
                "\n",
                "plt.suptitle('Model Predictions on Test Set', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "accuracy = sum(predictions == labels.numpy()) / len(labels) * 100\n",
                "print(f\"Batch Accuracy: {accuracy:.1f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5ï¸âƒ£ Confusion Matrix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import confusion_matrix\n",
                "import seaborn as sns\n",
                "\n",
                "# Get predictions on full test set\n",
                "all_preds = []\n",
                "all_labels = []\n",
                "\n",
                "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
                "\n",
                "for images, labels in test_loader:\n",
                "    preds, _, _ = predictor.predict_batch(images)\n",
                "    all_preds.extend(preds)\n",
                "    all_labels.extend(labels.numpy())\n",
                "\n",
                "# Create confusion matrix\n",
                "cm = confusion_matrix(all_labels, all_preds)\n",
                "\n",
                "plt.figure(figsize=(10, 8))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
                "            xticklabels=range(10), yticklabels=range(10))\n",
                "plt.xlabel('Predicted', fontsize=12)\n",
                "plt.ylabel('True', fontsize=12)\n",
                "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
                "plt.show()\n",
                "\n",
                "# Final accuracy\n",
                "accuracy = sum(np.array(all_preds) == np.array(all_labels)) / len(all_labels) * 100\n",
                "print(f\"\\nâœ… Test Set Accuracy: {accuracy:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6ï¸âƒ£ Analyzing Misclassifications"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find misclassified samples\n",
                "misclassified_idx = [i for i, (p, l) in enumerate(zip(all_preds, all_labels)) if p != l]\n",
                "\n",
                "print(f\"Total misclassified: {len(misclassified_idx)} out of {len(all_labels)}\")\n",
                "\n",
                "# Show some misclassified examples\n",
                "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
                "\n",
                "for i, ax in enumerate(axes.flat):\n",
                "    if i >= len(misclassified_idx):\n",
                "        ax.axis('off')\n",
                "        continue\n",
                "        \n",
                "    idx = misclassified_idx[i]\n",
                "    img = test_dataset[idx][0].squeeze().numpy()\n",
                "    img = img * 0.3081 + 0.1307\n",
                "    \n",
                "    ax.imshow(img, cmap='gray')\n",
                "    ax.set_title(f'Pred: {all_preds[idx]} | True: {all_labels[idx]}', color='#e74c3c', fontweight='bold')\n",
                "    ax.axis('off')\n",
                "\n",
                "plt.suptitle('Misclassified Examples', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸŽ‰ Summary\n",
                "\n",
                "In this notebook, we:\n",
                "\n",
                "1. âœ… **Explored the MNIST dataset** - 70K handwritten digit images\n",
                "2. âœ… **Built a CNN** - 3 conv layers + 3 FC layers (~300K params)\n",
                "3. âœ… **Trained the model** - Achieved ~99% accuracy\n",
                "4. âœ… **Made predictions** - Classified test images\n",
                "5. âœ… **Analyzed results** - Confusion matrix and misclassifications\n",
                "\n",
                "### Next Steps\n",
                "- Try the web interface: `python app.py`\n",
                "- Batch process images: `python batch_processor.py`\n",
                "- Experiment with different architectures"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}